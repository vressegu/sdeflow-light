# -*- coding: utf-8 -*-
"""Copy of sdeflow_equivalent_sdes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tx_Yt90NRgHve--ocIXi6SGR-0ebwH0N
"""


import time
import numpy as np
import torch
import torch.nn as nn
import sys
import os
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from sklearn.datasets import make_swiss_roll
from netCDF4 import Dataset
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
import seaborn as sns

from sde_scheme import euler_maruyama_sampler,heun_sampler,rk4_stratonovich_sampler
from own_plotting import plot_selected_inds
from SDEs import forward_SDE,SDE,VariancePreservingSDE,PluginReverseSDE,multiplicativeNoise
from data import ERA5,ncar_weather_station,weather_station,eof_pressure,Lorenz96,PODmodes,SwissRoll
from quantitative_comparison import compute_mmd
import gc

np.random.seed(0)
torch.manual_seed(0) 

DISPLAY_MAX_ROWS = 20  # number of max rows to print for a DataFrame
pd.set_option('display.max_rows', DISPLAY_MAX_ROWS)

# arguments

# Train
T0 = 1

MSGMs = [0,1]

ssm_intT_ref = False

num_steps_forward = 100
beta_min=1
beta_max=20
t_eps = 1/1000  
# default values from git repo
beta_min_SGM = 0.1
beta_max_SGM = 20
beta_maxs = [beta_max]

vtype = 'rademacher'
lr = 0.001
iterations = 10000
print_every = 1000

# Inference
num_stepss_backward = [1000,100,50,10]
include_t0_reverse = True # for plots
num_samples = 10000
# nruns_mmd = 1
nruns_mmd = 10

# Dataset
# datatype = 'swissroll'
datatype = 'POD'
# datatype = 'era5'

match datatype:
    case 'swissroll': # Swiss roll
        dims = [2]
        Res=[1]
    case 'POD': # POD
        dims = [2,4,8,16]
        Res=[300,3900]

        dims = [16]
        Res=[300,3900]

    # case 'lorenz':
 
    # case 'eof_pressure':        
 
    # case 'weather_station':
         
    # case 'ncar':
         
    case 'era5': # ERA5
        # dims = [2,4,8,16,32]
        # # dims = [2]
        # Res=[1]

        # ERA5-3var
        # dims = [3,6,9,18,30]
        dims = [30]
        Res=[1]

# # # DEBUG set:
# print('WARNING : DEBUG !!!!!!')
# iterations = 2
# iterations = 10000
# num_stepss_backward = [10]
# num_steps_forward = 10
# num_samples = 10
# batch_sizes = [2]

# Plots
scatter_plots = True
noising_plots = True
denoising_plots = True
save_results = True
plot_xlim = 3.0
height_seaborn = 1.2
ssize = height_seaborn
dpi=200

# Load results 
justLoad = False
justLoadmmmd = False
plt_show = False
print_RAM = False

if not justLoad:
    justLoadmmmd = False

if not plt_show:
    matplotlib.use("Agg")

#  Define models

### 2.2. Define MLP
class Swish(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return torch.sigmoid(x)*x

class MLP(nn.Module):
    def __init__(self,
                input_dim=2,
                index_dim=1,
                hidden_dim=128,
                act=Swish(),
                ):
        super().__init__()
        self.input_dim = input_dim
        self.index_dim = index_dim
        self.hidden_dim = hidden_dim
        self.act = act

        self.main = nn.Sequential(
            nn.Linear(input_dim+index_dim, hidden_dim),
            act,
            nn.Linear(hidden_dim, hidden_dim),
            act,
            nn.Linear(hidden_dim, hidden_dim),
            act,
            nn.Linear(hidden_dim, input_dim),
            )

    def forward(self, input, t):
        # init
        sz = input.size()
        input = input.view(-1, self.input_dim)
        t = t.view(-1, self.index_dim).float()

        # forward
        h = torch.cat([input, t], dim=1) # concat
        output = self.main(h) # forward
        return output.view(*sz)

### 2.3. Define evaluate function (compute ELBO)
@torch.no_grad()
def evaluate(gen_sde, x_test):
    gen_sde.eval()
    num_samples_ = x_test.size(0)
    test_elbo = gen_sde.elbo_random_t_slice(x_test)
    gen_sde.train()
    return test_elbo.mean(), test_elbo.std() / num_samples_ ** 0.5

# init device
if torch.cuda.is_available():
    device = 'cuda'
    print('use gpu\n')
# elif torch.backends.mps.is_available():
#     device = 'mps'
#     print('use mps\n')
else:
    device = 'cpu'
    print('use cpu\n')

if __name__ == '__main__':

    for beta_max in beta_maxs:
        mmd_SGM = torch.zeros((len(dims),len(Res),len(num_stepss_backward),nruns_mmd))
        mmd_MSGM = torch.zeros((len(dims),len(Res),len(num_stepss_backward),nruns_mmd))
        mmd_ref = torch.zeros((len(dims),len(Res),len(num_stepss_backward),nruns_mmd))
        i_MGMM = -1
        for MSGM in MSGMs:
            i_MGMM +=1
            plot_ylim_row = plot_xlim
            plot_xlim_col = plot_xlim

            if not MSGM:
                normalized_data = True
            else:
                normalized_data = False

            i_dims = -1
            for dim in dims:
                i_dims +=1
                
                i_Res = -1
                for Re in Res:
                    i_Res +=1

                    np.random.seed(0)
                    torch.manual_seed(0) 

                    num_samples_init = min(int(1e6),iterations*batch_sizes[-1])

                    ## 1. Initialize dataset
                    match datatype:
                        case 'swissroll':
                            sampler = SwissRoll()
                            normalized_data = False
                        case 'POD':
                            sampler = PODmodes(Re,dim, normalized=normalized_data)
                        case 'lorenz':
                            sampler = Lorenz96(Re,dim, normalized=normalized_data)
                        case 'eof_pressure':
                            sampler = eof_pressure(dim)
                        case 'weather_station':
                            sampler = weather_station(dim) 
                        case 'ncar':
                            sampler = ncar_weather_station(dim) 
                        case 'era5':
                            sampler = ERA5(dim, variables = ["10m_u_component_of_wind", "10m_v_component_of_wind", "vorticity"]) 
                            normalized_data = False
                            columns=["$u$, Berlin", "$v$, Berlin", "$\omega$, Berlin",\
                                     "$u$, Paris", "$v$, Paris", "$\omega$, Paris"]

                    with torch.no_grad():
                        xtest = sampler.sampletest(num_samples)
                        sampler.dim = xtest.shape[1]
                        std_test = xtest.std(axis=0)
                        if normalized_data:
                            std_norm = sampler.get_std()
                        else:
                            std_norm = torch.ones((xtest.shape[1]))

                        plt.close('all')
                        dimplot_max = 8
                        dimplot = np.min([dimplot_max,xtest.shape[1]])

                        if (datatype == 'era5') and xtest.shape[1]>= 9:
                            dimplot = 6
                            pddatatest = pd.DataFrame(torch.cat( ((std_norm * xtest)[:,6:9], \
                                                                    (std_norm * xtest)[:,0:3]),dim=1).to('cpu').data.numpy(), \
                                                    columns=columns \
                                                    )
                        else:
                            pddatatest = pd.DataFrame((std_norm * xtest).data.numpy()[:,0:dimplot], columns=range(1,1+dimplot))

                        plot_kws={"s": ssize}
                        scatter = sns.pairplot(pddatatest, aspect=1, height=height_seaborn, corner=True,plot_kws=plot_kws)
                        plt.tight_layout()
                        if plt_show:
                            plt.show(block=False)   
                            plt.pause(0.1)
                        plt.savefig("results/" + sampler.name + ".png", dpi=dpi)
                        plt.close()
                        plt.pause(0.1)
                        plt.close('all')
                        
                        if (datatype == 'era5') and xtest.shape[1]>= 9:
                            pddatatrain = pd.DataFrame(torch.cat( ((std_norm * sampler.sample(num_samples_init).to('cpu'))[:,6:9], \
                                                                    (std_norm * sampler.sample(num_samples_init).to('cpu'))[:,0:3]),dim=1).to('cpu').data.numpy(), \
                                                    columns=columns \
                                                    )
                        else:
                            pddatatrain = pd.DataFrame((std_norm *  sampler.sample(num_samples_init)).data.numpy()[:,0:dimplot], columns=range(1,1+dimplot))

                        plot_kws={"s": ssize}
                        scatter = sns.pairplot(pddatatrain, aspect=1, height=height_seaborn, corner=True,plot_kws=plot_kws)

                        plt.tight_layout()
                        if plt_show:
                            plt.show(block=False)   
                            plt.pause(0.1)
                        plt.savefig("results/" + sampler.name + "_train.png", dpi=dpi)
                        plt.close()
                        del scatter, pddatatrain
                        plt.pause(0.1)
                    

                    ## 3. Train

                    # iterationss = [100000, 10000, 1000, 100, 10]
                    # for iterations in iterationss:
                    # batch_sizes = [256, 128, 64, 32, 16, 8, 4]

                    # nosamples = np.linspace( n0, sampler.max_nsamples, sampler.max_nsamples/10)  1000 2000 3000.... 10000
                    # if iter = k * 1000     (assume iterations = 10000)
                    #  -> choose nosamples[k] samples
                    # Technique to "generate" new sampler : boostrapping
                    
                    xtest = xtest.to(device)             

                    for batch_size in batch_sizes:
                        num_samples_init = min(int(1e6),iterations*batch_size)
                    
                        # init models
                        drift_q = MLP(input_dim=sampler.dim, index_dim=1, hidden_dim=128).to(device)
                        T = torch.nn.Parameter(torch.FloatTensor([T0]), requires_grad=False)


                        with torch.no_grad():
                            if MSGM:
                                x_init = sampler.sample(num_samples_init).to(device)
                                inf_sde = multiplicativeNoise(x_init,beta_min=beta_min, beta_max=beta_max, \
                                                              t_epsilon=t_eps, T=T, num_steps_forward=num_steps_forward, \
                                                              device=device, estim_cst_norm_dens_r_T = False)
                                del x_init
                            else:
                                inf_sde = VariancePreservingSDE(beta_min=beta_min_SGM, beta_max=beta_max_SGM, \
                                                                t_epsilon=t_eps, T=T, num_steps_forward=num_steps_forward, \
                                                                device=device)
                        gen_sde = PluginReverseSDE(inf_sde, drift_q, T, vtype=vtype, debias=False, ssm_intT=ssm_intT).to(device)

                        print("data = " + sampler.name )
                        print("name_SDE = " + str(inf_sde.name_SDE) )   
                        print("num_steps_forward = " + str(num_steps_forward))
                        print("beta_min = " + str(beta_min))
                        print("beta_max = " + str(beta_max))
                        print("t_eps = " + str(t_eps))     
                        print("iterations = " + str(iterations) )
                        print("batch_size = " + str(batch_size) ) 
                        print("ssm_intT = " + str(ssm_intT) )  

                        folder_results = "results"
                        name_simu_root = sampler.name + "_" \
                            + gen_sde.base_sde.name_SDE + "_" + str(iterations) + "iteLearning_" \
                            + str(batch_size) + "batchSize_" \
                            + str(num_steps_forward) + "stepsForw_" \
                            + str(beta_min) + "beta_min" \
                            + str(beta_max) + "beta_max" 
                        if ssm_intT:
                            name_simu_root += "_intLoss"
                        
                        # Forward SDE
                        with torch.no_grad():
                            print('integrate forward SDE')
                            for_sde = forward_SDE(inf_sde, T)
                            xs_forward = rk4_stratonovich_sampler(for_sde, xtest.clone(), num_steps_forward,  \
                                                                lmbd=0., keep_all_samples=True, \
                                                                include_t0=True, norm_correction = MSGM) # sample
                            xgen_forward = xs_forward[-1,:,:].to(device)
                            cov_xtest = torch.cov(xtest.T)
                            cov_xgen_forward = torch.cov(xgen_forward.T)
                            xgen_forward_var = torch.var(xgen_forward.T,dim=1)
                            xgen_forward_var_mean = xgen_forward_var.mean()

                            xtest_var = torch.var(xtest.T,dim=1)
                            xtest_var_mean = xtest_var.mean()
                            cov_xgen_forward_converged = xtest_var_mean * torch.eye(sampler.dim).to(device)
                            # since tr(cov)=E||X||^2 is theoretically conserved
                            d_cov_xtest = torch.norm(cov_xtest - cov_xgen_forward_converged)/torch.norm(cov_xgen_forward_converged)
                            d_cov_xgen_forward = torch.norm(cov_xgen_forward - cov_xgen_forward_converged)/torch.norm(cov_xgen_forward_converged)
                            print("dist cov_xtest to  cov_xgen_forward_converged = " + str(d_cov_xtest.item()))
                            print("dist cov_xgen_forward  to  cov_xgen_forward_converged = " + str(d_cov_xgen_forward.item()))

                            # indices to visualize
                            fig_step = int(num_steps_forward/10) #100
                            if fig_step < 1:
                                fig_step = 1
                            inds_forward = range(0, num_steps_forward+1, fig_step)
                            if (noising_plots):
                                plot_selected_inds(xs_forward, inds_forward, \
                                    use_xticks= True, use_yticks=False, lmbd = 0., \
                                    include_t0=True, backward=False,
                                    plt_show=plt_show) # plot
                                time.sleep(0.5)
                                if plt_show:
                                    plt.show(block=False)
                                name_fig = folder_results + "/" + name_simu_root + "_Forward.png" 
                                plt.savefig(name_fig)
                                if plt_show:
                                    plt.pause(1)
                                plt.close()
                                plt.close('all')

                        del xs_forward, xgen_forward, cov_xgen_forward, cov_xgen_forward_converged, xgen_forward_var, xgen_forward_var_mean
                        del for_sde
                        gc.collect()

                        if (not justLoad):
                            # init optimizer
                            optim = torch.optim.Adam(gen_sde.parameters(), lr=lr)

                            # train
                            start_time = time.time()
                            for i in range(iterations):
                                optim.zero_grad() # init optimizer
                                with torch.no_grad():
                                    x = sampler.sample(batch_size).to(device) # sample data
                                loss = gen_sde.ssm(x).mean() # forward and compute loss
                                loss.backward() # backward
                                optim.step() # update

                                # print
                                if (i == 0) or ((i+1) % print_every == 0):
                                    # elbo
                                    elbo, elbo_std = evaluate(gen_sde, x)

                                    # print
                                    elapsed = time.time() - start_time
                                    print('| iter {:6d} | {:5.2f} ms/step | loss {:8.3f} | elbo {:8.3f} | elbo std {:8.3f} '
                                        .format(i+1, elapsed*1000/print_every, loss.item(), elbo.item(), elbo_std.item()))
                                    start_time = time.time()

                                    del elbo, elbo_std
                                    gc.collect()
                                
                                del x
                                # torch.mps.empty_cache()  # does not do much on MPS, but still good practice
                                # gc.collect()
                            del loss, optim
                            gc.collect()


                        ## 4. Visualize
                        with torch.no_grad():

                            ### 4.3. Simulate SDEs
                            """
                            Simulate the generative SDE by using RK4 method
                            """
                            i_num_stepss_backward = -1
                            for num_steps_backward in num_stepss_backward:
                                i_num_stepss_backward +=1
                                print("Generation : num_steps_backward = " + str(num_steps_backward))
                                # init param
                                # num_samples = 100000

                                # indices to visualize
                                fig_step = int(num_steps_backward/10) #100
                                if fig_step < 1:
                                    fig_step = 1
                                if include_t0_reverse:
                                    inds = range(0, num_steps_backward+1, fig_step)
                                else:
                                    inds = range(fig_step-1, num_steps_backward, fig_step)
                                # sample and plot
                                plt.close('all')
                                lmbd = 0.
                                name_simu = folder_results + "/" + name_simu_root \
                                    + str(t_eps) + "t_eps" \
                                    + str(num_steps_backward) + "stepsBack_" \
                                    + str(include_t0_reverse) + "t0infer"
                                
                                for i_run in range(nruns_mmd):
                                    print("Run number : " + str(i_run))
                                    if i_run > 0 :
                                        name_simu = "runs/" + name_simu_root \
                                            + str(t_eps) + "t_eps" \
                                            + str(num_steps_backward) + "stepsBack_" \
                                            + str(include_t0_reverse) + "t0infer" \
                                            + "_run"+ str(i_run)
                                    
                                    if (justLoad):
                                        save_results = False
                                        xs = torch.load(name_simu + ".pt", weights_only=True)
                                    else:
                                        x_0 = gen_sde.latent_sample(num_samples, sampler.dim) # init from prior
                                        xs = rk4_stratonovich_sampler(gen_sde, x_0, num_steps_backward, lmbd=lmbd,\
                                                                    keep_all_samples=True, 
                                                                    include_t0=include_t0_reverse, 
                                                                    norm_correction = MSGM) # sample
                                        del x_0
                                        if (save_results):
                                            torch.save(xs, name_simu + ".pt")
                                    xgen = xs[-1,:,:].to(device)

                                    if save_results and not justLoad:
                                        np.save(name_simu + ".pt", xgen.clone().detach().cpu().numpy())

                                    # Identify rows with NaN values
                                    nan_mask = (torch.isnan(xgen) | (torch.abs(xgen) > 1e3 )).any(dim=1)
                                    # Count rows with NaN values
                                    nan_count = nan_mask.sum().item()
                                    if nan_count > 0:
                                        print(f"Number of rows with NaN or large value: {nan_count}")
                                    # Remove rows with NaN values
                                    xgen = xgen[~nan_mask,:]
                                    del nan_mask


                                    # MMD
                                    if not justLoadmmmd:
                                        with torch.no_grad():
                                            x_mmd1 = sampler.sample(xtest.shape[0]).to(device)
                                            x_mmd2 = sampler.sample(xtest.shape[0]).to(device)
                                            dist_train_to_test = compute_mmd(std_norm * x_mmd1,std_norm * xtest)
                                            dist_test_to_test = compute_mmd(std_norm * x_mmd1,std_norm * xtest)
                                            dist = compute_mmd(std_norm * xgen,std_norm * xtest)
                                        mmd_ref[i_dims, i_Res, i_num_stepss_backward,i_run] = dist_train_to_test
                                        if MSGM:
                                            mmd_MSGM[i_dims, i_Res, i_num_stepss_backward,i_run] = dist
                                        else:
                                            mmd_SGM[i_dims, i_Res, i_num_stepss_backward,i_run] = dist
                                        del dist

                                    if (scatter_plots) and (i_run == 0):

                                        if (datatype == 'era5') and xtest.shape[1]>= 9:
                                            pddatagen = pd.DataFrame(torch.cat( ((std_norm * xgen)[:,6:9],(std_norm * xgen)[:,0:3]),dim=1).to('cpu'), \
                                                                     columns=columns \
                                                                    )
                                        else:
                                            pddatagen = pd.DataFrame((std_norm * xgen)[:,0:dimplot].to('cpu'), columns=range(1,1+dimplot))

                                        pddata = pd.concat([pddatatest.assign(samples="test"), pddatagen.assign(samples="gen.")])

                                        plot_kws={'alpha':0.1, "s": ssize}
                                        scatter = sns.pairplot(pddata, kind='scatter', hue="samples", aspect=1, height=height_seaborn, corner=True,plot_kws=plot_kws)
                                        handles = scatter._legend_data.values()
                                        labels = scatter._legend_data.keys()
                                        scatter.figure.legend(handles=handles, labels=labels, loc='upper right', markerscale=5*ssize )
                                        scatter._legend.remove()

                                        for i, row in enumerate(scatter.axes):
                                            plot_ylim_row = plot_xlim * std_norm[i]* std_test[i]
                                            for j, ax in enumerate(row):
                                                plot_xlim_col = plot_xlim * std_norm[j]* std_test[j]
                                                if ax is not None:
                                                    if i == j:  # Diagonal
                                                        ax.set_xlim((-plot_xlim_col,plot_xlim_col))
                                                    if j < i:  # since corner=True, we only have lower triangle
                                                        ax.set_xlim((-plot_xlim_col,plot_xlim_col))
                                                        ax.set_ylim((-plot_ylim_row,plot_ylim_row))
                                        plt.tight_layout()
                                        time.sleep(0.5)
                                        if plt_show:
                                            plt.show(block=False)
                                            plt.pause(1)
                                        plt.tight_layout()
                                        # plt.show()
                                        time.sleep(0.5)
                                        if plt_show:
                                            plt.show(block=False)
                                        name_fig = name_simu + "_multDim.png" 
                                        plt.savefig(name_fig, dpi=dpi)
                                        if plt_show:
                                            plt.pause(1)
                                        plt.close()
                                        del pddatagen, pddata, scatter

                                    if (denoising_plots) and (i_run == 0):
                                        plot_selected_inds(xs, inds, True, False, lmbd, include_t0=include_t0_reverse, plt_show=plt_show) # plot
                                        time.sleep(0.5)
                                        if plt_show:
                                            plt.show(block=False)
                                        name_fig = name_simu + ".png" 
                                        plt.savefig(name_fig)
                                        if plt_show:
                                            plt.pause(1)
                                        plt.close()
                                        plt.close('all')

                                    del xs, xgen
                                    gc.collect()

                            if justLoadmmmd and (not MSGM):
                                mmd_SGM = torch.load(folder_results + "/" + name_simu_root + "_globalMMDfile_SGM_" + str(nruns_mmd) + "runs.pt")
                                mmd_MSGM = torch.load(folder_results + "/" + name_simu_root + "_globalMMDfile_MSGM_" + str(nruns_mmd) + "runs.pt") 
                                mmd_ref = torch.load(folder_results + "/" + name_simu_root + "_globalMMDfile_ref_" + str(nruns_mmd) + "runs.pt") 

                            fig = plt.figure(figsize=(5,3))
                            
                            mmmd_SGM = mmd_SGM.mean(dim=3)
                            q10mmd_SGM = mmd_SGM.quantile(0.1,dim=3)
                            q90mmd_SGM = mmd_SGM.quantile(0.9,dim=3)
                            mmmd_MSGM = mmd_MSGM.mean(dim=3)
                            q10mmd_MSGM = mmd_MSGM.quantile(0.1,dim=3)
                            q90mmd_MSGM = mmd_MSGM.quantile(0.9,dim=3)
                            mmmd_ref = mmd_ref.mean(dim=3)
                            q10mmd_ref = mmd_ref.quantile(0.1,dim=3)
                            q90mmd_ref = mmd_ref.quantile(0.9,dim=3)

                            alpha_plot = 0.2
                            i_num_stepss_backward = range(len(num_stepss_backward))
                            plt.loglog(num_stepss_backward,mmmd_SGM[i_dims,i_Res,i_num_stepss_backward].flatten(),label='SGM')
                            plt.fill_between(num_stepss_backward, q10mmd_SGM[i_dims,i_Res,i_num_stepss_backward].flatten(), q90mmd_SGM[i_dims,i_Res,i_num_stepss_backward].flatten(),
                                alpha=alpha_plot)
                            plt.loglog(num_stepss_backward,mmmd_MSGM[i_dims,i_Res,i_num_stepss_backward].flatten(),label='MSGM')
                            plt.fill_between(num_stepss_backward, q10mmd_MSGM[i_dims,i_Res,i_num_stepss_backward].flatten(), q90mmd_MSGM[i_dims,i_Res,i_num_stepss_backward].flatten(),
                                alpha=alpha_plot)
                            plt.loglog(num_stepss_backward,mmmd_ref[i_dims,i_Res,i_num_stepss_backward].flatten(),label='train data')
                            plt.fill_between(num_stepss_backward, q10mmd_ref[i_dims,i_Res,i_num_stepss_backward].flatten(), q90mmd_ref[i_dims,i_Res,i_num_stepss_backward].flatten(),
                                alpha=alpha_plot)
                            plt.legend()
                            plt.ylabel('MMD')
                            plt.xlabel('nb timesteps in backward SDE')
                            plt.xticks(num_stepss_backward)
                            plt.tight_layout()
                            if plt_show:
                                plt.show(block=False)
                            name_fig = folder_results + "/" + name_simu_root + "_MMD_" + str(nruns_mmd) + "runs.png" 
                            plt.savefig(name_fig)
                            if plt_show:
                                plt.pause(1)
                            plt.close(fig)
                            plt.close()
                            del fig

                            if mmd_SGM.shape[0]>1:
                                range_dims = range(len(dims))
                                fig = plt.figure(figsize=(5,3))
                                plt.loglog(dims,mmmd_SGM[range_dims,i_Res,0].flatten(),label='SGM')
                                plt.fill_between(dims, q10mmd_SGM[range_dims,i_Res,0].flatten(), q90mmd_SGM[range_dims,i_Res,0].flatten(),
                                    alpha=alpha_plot)
                                plt.loglog(dims,mmmd_MSGM[range_dims,i_Res,0].flatten(),label='MSGM')
                                plt.fill_between(dims, q10mmd_MSGM[range_dims,i_Res,0].flatten(), q90mmd_MSGM[range_dims,i_Res,0].flatten(),
                                    alpha=alpha_plot)
                                plt.loglog(dims,mmmd_ref[range_dims,i_Res,0].flatten(),label='train data')
                                plt.fill_between(dims, q10mmd_ref[range_dims,i_Res,0].flatten(), q90mmd_ref[range_dims,i_Res,0].flatten(),
                                    alpha=alpha_plot)
                                plt.legend()
                                plt.ylabel('MMD')
                                plt.xlabel('dimension')
                                plt.xticks(dims)
                                plt.tight_layout()
                                if plt_show:
                                    plt.show(block=False)
                                name_fig = folder_results + "/" + name_simu_root + "_MMD_withDim_" + str(nruns_mmd) + "runs.png" 
                                plt.savefig(name_fig)
                                if plt_show:
                                    plt.pause(1)
                                plt.close(fig)
                                plt.close()
                                del fig

                        del gen_sde, inf_sde, drift_q, T
                        gc.collect()

                    del pddatatest, cov_xtest, xtest, std_test, xtest_var, xtest_var_mean
                    del sampler
                    gc.collect()

        if not justLoadmmmd:
            torch.save(mmd_SGM, folder_results + "/" + name_simu_root + "_globalMMDfile_SGM_" + str(nruns_mmd) + "runs.pt")
            torch.save(mmd_MSGM, folder_results + "/" + name_simu_root + "_globalMMDfile_MSGM_" + str(nruns_mmd) + "runs.pt")
            torch.save(mmd_ref, folder_results + "/" + name_simu_root + "_globalMMDfile_ref_" + str(nruns_mmd) + "runs.pt")

