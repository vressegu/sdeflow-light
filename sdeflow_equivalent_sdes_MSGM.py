# -*- coding: utf-8 -*-
"""Copy of sdeflow_equivalent_sdes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tx_Yt90NRgHve--ocIXi6SGR-0ebwH0N
"""


import time
import numpy as np
import torch
import torch.nn as nn
import sys
import os
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_swiss_roll
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from sde_scheme import euler_maruyama_sampler,heun_sampler,rk4_stratonovich_sampler
from own_plotting import plot_selected_inds
from SDEs import VariancePreservingSDE,PluginReverseSDE,multiplicativeNoise


if __name__ == '__main__':
    
    ## 1. Initialize dataset
    class PODmodes:
        def __init__(self):
            self.dim = 8

            pathData = '../MultiplicativeDiffusion/'
            pathData = pathData + 'tempPODModes/LES_Re300/temporalModes_16modes'
            # pathData = pathData + 'tempPODModes/LES_Re3900/temporalModes_16modes'
            npdata = np.load(pathData + '/U.npy')
            npdatatest = np.load(pathData + '_test/U.npy')
            # npdata = np.load(pathData + 'tempPODModes/LES_Re3900/temporalModes_16modes/U.npy')
            self.npdata = npdata[:,0:self.dim]
            self.npdatatest = npdatatest[:,0:self.dim]

            self.max_nsamples = npdatatest.shape[0]

        def sample(self, n):               
            assert (n <= self.npdata.shape[0])
            idx = np.random.randint(0,self.npdata.shape[0], size = n)
            return torch.from_numpy(self.npdata[idx,:])
            # return torch.from_numpy( )
                # make_swiss_roll(n)[0][:, [0, 2]].astype('float32') / 5.) # Changed: Pass noise as a keyword argument


    class SwissRoll:
        """
        Swiss roll distribution sampler.
        noise control the amount of noise injected to make a thicker swiss roll
        """
        def __init__(self): 
            self.dim = 2
        def sample(self, n, noise=0.5):
            if noise is None:
                noise = 0.5
            return torch.from_numpy(
                make_swiss_roll(n, noise=noise)[0][:, [0, 2]].astype('float32') / 5.) # Changed: Pass noise as a keyword argument

    # sampler = SwissRoll()
    # x = sampler.sample(100000).data.numpy()
    sampler = PODmodes()
    x = sampler.sample(sampler.max_nsamples).data.numpy()
    plt.close('all')
    fig = plt.figure(figsize=(5, 5))

    # pddata = pd.DataFrame(npdata, columns=['A', 'B', 'C', 'D'])
    pddata = pd.DataFrame(x, columns=range(1,1+x.shape[1]))

    pd.plotting.scatter_matrix(pddata, diagonal="kde")
    plt.tight_layout()
    # plt.show()

    # _ = plt.hist2d(x[:,0], x[:,1], 200, range=((-5,5), (-5,5)))
    # plt.axis('off')
    # plt.tight_layout()

    plt.show(block=False)
    plt.savefig("swissroll.png")
    plt.pause(0.1)
    plt.close()
    

    ## 2. Define models

    ### 2.2. Define MLP
    class Swish(nn.Module):
        def __init__(self):
            super().__init__()

        def forward(self, x):
            return torch.sigmoid(x)*x

    class MLP(nn.Module):
        def __init__(self,
                    input_dim=2,
                    index_dim=1,
                    hidden_dim=128,
                    act=Swish(),
                    ):
            super().__init__()
            self.input_dim = input_dim
            self.index_dim = index_dim
            self.hidden_dim = hidden_dim
            self.act = act

            self.main = nn.Sequential(
                nn.Linear(input_dim+index_dim, hidden_dim),
                act,
                nn.Linear(hidden_dim, hidden_dim),
                act,
                nn.Linear(hidden_dim, hidden_dim),
                act,
                nn.Linear(hidden_dim, input_dim),
                )

        def forward(self, input, t):
            # init
            sz = input.size()
            input = input.view(-1, self.input_dim)
            t = t.view(-1, self.index_dim).float()

            # forward
            h = torch.cat([input, t], dim=1) # concat
            output = self.main(h) # forward
            return output.view(*sz)

    ### 2.3. Define evaluate function (compute ELBO)
    @torch.no_grad()
    def evaluate(gen_sde, x_test):
        gen_sde.eval()
        num_samples = x_test.size(0)
        test_elbo = gen_sde.elbo_random_t_slice(x_test)
        gen_sde.train()
        return test_elbo.mean(), test_elbo.std() / num_samples ** 0.5

    ## 3. Train
    # init device
    if torch.cuda.is_available():
        device = 'cuda'
        print('use gpu\n')
    elif torch.backends.mps.is_available():
        device = 'mps'
        print('use mps\n')
    else:
        device = 'cpu'
        print('use cpu\n')

    # iterationss = [100000, 10000, 1000, 100, 10]
    iterationss = [10000]
    # for iterations in iterationss:
    batch_sizes = [256, 128, 64, 32, 16, 8, 4]
    for batch_size in batch_sizes:

        # arguments
        T0 = 1
        vtype = 'rademacher'
        lr = 0.001
        # batch_size = 256
        # #iterations = 100000
        iterations = 10000
        # print_every = 50
        print_every = 1000

        # init models
        # drift_q = MLP(input_dim=2, index_dim=1, hidden_dim=128).to(device)
        drift_q = MLP(input_dim=sampler.dim, index_dim=1, hidden_dim=128).to(device)
        T = torch.nn.Parameter(torch.FloatTensor([T0]), requires_grad=False)
        x_init = sampler.sample(iterations*batch_size).data.numpy()
        # inf_sde = multiplicativeNoise(x_init,beta=1, T=T).to(device)
        inf_sde = VariancePreservingSDE(beta_min=1, beta_max=1, T=T).to(device)
        # # # inf_sde = VariancePreservingSDE(beta_min=0.1, beta_max=20.0, T=T).to(device)
        gen_sde = PluginReverseSDE(inf_sde, drift_q, T, vtype=vtype, debias=False).to(device)

        print("iterations = " + str(iterations) )
        print("name_SDE = " + inf_sde.name_SDE )

        # init optimizer
        optim = torch.optim.Adam(gen_sde.parameters(), lr=lr)

        # train
        start_time = time.time()
        for i in range(iterations):
            optim.zero_grad() # init optimizer
            x = sampler.sample(batch_size).to(device) # sample data
            loss = gen_sde.ssm(x).mean() # forward and compute loss
            loss.backward() # backward
            optim.step() # update

            # print
            if (i == 0) or ((i+1) % print_every == 0):
                # elbo
                elbo, elbo_std = evaluate(gen_sde, x)

                # print
                elapsed = time.time() - start_time
                print('| iter {:6d} | {:5.2f} ms/step | loss {:8.3f} | elbo {:8.3f} | elbo std {:8.3f} '
                    .format(i+1, elapsed*1000/print_every, loss.item(), elbo.item(), elbo_std.item()))
                start_time = time.time()


        ## 4. Visualize

        ### 4.3. Simulate SDEs
        """
        Simulate the generative SDE by using RK4 method
        """
        num_stepss = [1000, 100, 50, 20, 10, 5, 3, 2]
        for num_steps in num_stepss:
            print("Generation : num_steps = " + str(num_steps))
            # init param
            num_samples = 100000

            # lambdas
            lmbds = [0.]
            # lmbds = [0., 1.0]

            # indices to visualize
            num_figs = 10
            if num_figs > num_steps:
                num_figs = num_steps
            fig_step = int(num_steps/50) #100
            if fig_step < 1:
                fig_step = 1
            inds = [i-1 for i in range(num_steps-(num_figs-1)*fig_step, num_steps+1, fig_step)]

            # sample and plot
            plt.close('all')
            for lmbd in lmbds:
                x_0 = gen_sde.latent_sample(num_samples, 2, device=device) # init from prior
                # xs = euler_maruyama_sampler(gen_sde, x_0, num_steps, lmbd=lmbd) # sample
                # xs = heun_sampler(gen_sde, x_0, num_steps, lmbd=lmbd) # sample
                xs = rk4_stratonovich_sampler(gen_sde, x_0, num_steps, lmbd=lmbd) # sample
                plot_selected_inds(xs, inds, True, True, lmbd) # plot
                time.sleep(0.5)
                plt.show(block=False)
                name_fig = gen_sde.base_sde.name_SDE + "_" + str(iterations) + "iteLearning_" \
                    + str(batch_size) + "batchSize_" \
                    + str(num_steps) + "stepsBack_lmbd=" + str(lmbd) + ".png" 
                plt.savefig(name_fig)
                plt.pause(1)
                plt.close()
